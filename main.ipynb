{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trail2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v73o8i1zGlk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from IPython.display import display\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow.keras.backend as K\n",
        "import numpy as np\n",
        "import os\n",
        "import gc\n",
        "import glob\n",
        "from PIL import Image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.layers import BatchNormalization, Dense, Flatten, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
        "from sklearn.utils import class_weight\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MK0HO2H6zLam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tf_reset_graph(model=None):\n",
        "    tf.reset_default_graph()\n",
        "    K.clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "def tf_reset_callbacks(checkpoint=None, reduce_lr=None, early_stopping=None, tensorboard=None):\n",
        "    checkpoint = None\n",
        "    reduce_lr = None\n",
        "    early_stopping = None\n",
        "    tensorboard = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-uCa1qjWsH4",
        "colab_type": "text"
      },
      "source": [
        "Flag to decide whether to use colab or not\n",
        "\n",
        "Also prints out the platform processor if Colab is used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cISUrE_NzN9T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "1c6d2497-e242-4c17-facf-e57326c1288f"
      },
      "source": [
        "use_colab = True\n",
        "if use_colab:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  proj_path = 'drive/My Drive/Colab/Workspace/'\n",
        "  import platform\n",
        "  print(\"Processor: \", platform.processor())\n",
        "  !nvidia-smi\n",
        "else:\n",
        "  proj_path=''"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Processor:  x86_64\n",
            "Wed Sep 25 21:26:33 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.40       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P0    85W / 149W |   8428MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfh3pZTHW6lp",
        "colab_type": "text"
      },
      "source": [
        "Describing the paths to the train, test and validation datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWEpxO9nzUEa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "87b7c86f-1455-489c-ef5f-b413a9b34cba"
      },
      "source": [
        "image_ext = '.jpeg'\n",
        "data_dir = proj_path+'dataset/'\n",
        "test_dir = data_dir+'test/'\n",
        "train_dir = data_dir+'train/'\n",
        "val_dir = data_dir+'val/'\n",
        "normals = 'NORMAL/'\n",
        "infected = 'PNEUMONIA/'\n",
        "training_normal_nos = len(glob.glob1(train_dir+normals,'*'+image_ext))\n",
        "training_infected_nos = len(glob.glob1(train_dir+infected,'*'+image_ext))\n",
        "training_nos = training_infected_nos+training_normal_nos\n",
        "testing_normal_nos = len(glob.glob1(test_dir+normals,'*'+image_ext))\n",
        "testing_infected_nos = len(glob.glob1(test_dir+infected,'*'+image_ext))\n",
        "testing_nos = testing_infected_nos+testing_normal_nos\n",
        "validation_normal_nos = len(glob.glob1(val_dir+normals,'*'+image_ext))\n",
        "validation_infected_nos = len(glob.glob1(val_dir+infected,'*'+image_ext))\n",
        "validation_nos = validation_infected_nos+validation_normal_nos\n",
        "\n",
        "print('Total number of training images: ', training_nos)\n",
        "print('Total number of testing images: ', testing_nos)\n",
        "print('Total number of validation images: ', validation_nos)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of training images:  5230\n",
            "Total number of testing images:  624\n",
            "Total number of validation images:  16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TI9ChFOMXCB-",
        "colab_type": "text"
      },
      "source": [
        "A custom function to create ImageDataGenerator while using images from the 'path'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xGAlzwk0Ff9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_image_data_generator(target_size, rescale, path, batch_size, class_mode = 'categorical', shuffle=True, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True):\n",
        "\n",
        "    data_generator = ImageDataGenerator(rescale=rescale,\n",
        "                                        shear_range=shear_range,\n",
        "                                        zoom_range=zoom_range,\n",
        "                                        horizontal_flip=horizontal_flip)\n",
        "\n",
        "    return data_generator.flow_from_directory(path,\n",
        "                                       class_mode=class_mode,\n",
        "                                       target_size=target_size,\n",
        "                                       shuffle=shuffle,\n",
        "                                       batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o6JtU0cXN5B",
        "colab_type": "text"
      },
      "source": [
        "rescaling and defining target size, class mode is kept categorical, although one can also use binary since it's a binary classification problem.\n",
        "\n",
        "test, train and validation image data generators are created"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktRnveaW03qD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "dd1fb29e-f555-4468-c115-4dde3cd567e5"
      },
      "source": [
        "rescale_factor = 1./255\n",
        "target_size = (150,150)\n",
        "class_mode = 'categorical'\n",
        "training_batch_size = 163\n",
        "train_data_generator = create_image_data_generator(target_size=target_size, \n",
        "                                                   rescale=rescale_factor, \n",
        "                                                   path=train_dir, \n",
        "                                                   batch_size=training_batch_size)\n",
        "\n",
        "test_data_generator = create_image_data_generator(target_size=target_size,\n",
        "                                                 rescale=rescale_factor,\n",
        "                                                 path=test_dir,\n",
        "                                                 batch_size=testing_nos)\n",
        "\n",
        "val_data_generator = create_image_data_generator(target_size=target_size,\n",
        "                                                 rescale=rescale_factor,\n",
        "                                                 path=val_dir,\n",
        "                                                 batch_size=validation_nos)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 5230 images belonging to 2 classes.\n",
            "Found 624 images belonging to 2 classes.\n",
            "Found 16 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo7IBh8xXlkC",
        "colab_type": "text"
      },
      "source": [
        "Callbacks are described below. earlystopping has been removed since the model stopped too early due to bad val_loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yttRLH4-06f3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_reset_callbacks()\n",
        "tf_reset_graph()\n",
        "base = proj_path+'bin'\n",
        "model_dir = os.path.join(base,'models')\n",
        "log_dir = os.path.join(base,'logs')\n",
        "model_file = model_dir + \"{epoch:02d}-val_acc-{val_acc:.2f}-val_loss-{val_loss:.2f}.hdf5\"\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    model_file, \n",
        "    monitor='val_acc', \n",
        "    save_best_only=True)\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    verbose=1,\n",
        "    restore_best_weights=True)\n",
        "\n",
        "tensorboard = TensorBoard(\n",
        "    log_dir=log_dir,\n",
        "    batch_size=training_batch_size,\n",
        "    update_freq = 'batch')\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    cooldown=2,\n",
        "    min_lr=1e-10,\n",
        "    verbose=1)\n",
        "\n",
        "#callbacks = [checkpoint, reduce_lr, early_stopping, tensorboard]\n",
        "callbacks = [checkpoint, reduce_lr, tensorboard]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbaCKDPNXu8A",
        "colab_type": "text"
      },
      "source": [
        "InceptionV3 has bee used with non-trainable weights\n",
        "Moreover a extra layers are added in the following manner:\n",
        "Dense(512)->relu,\n",
        "Dropout(0.15),\n",
        "Dense(256)->relu,\n",
        "Flatten(),\n",
        "Dense(2)-> sigmoid (since 2 classes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufInFYnz1DA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "    inception_model = InceptionV3(weights='imagenet', include_top=False, input_shape = target_size+(3,))\n",
        "    x = inception_model.output\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Flatten()(x)\n",
        "    predictions = Dense(2,activation='softmax')(x)\n",
        "    model = Model(inputs = inception_model.input, outputs=predictions)\n",
        "    for layer in inception_model.layers:\n",
        "        layer.trainable = False\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8LLO_hVYNP8",
        "colab_type": "text"
      },
      "source": [
        "model is compiled, SGD optimizer is used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnVIktlt1MwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "steps_per_epoch = len(train_data_generator)\n",
        "validation_steps = len(val_data_generator)\n",
        "classes = train_data_generator.classes\n",
        "class_weights = class_weight.compute_class_weight('balanced', np.unique(classes), classes)\n",
        "\n",
        "optimizer = SGD()#Adam()\n",
        "model = create_model()\n",
        "model.compile(optimizer=optimizer, loss = 'categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Efv_L00K1QT9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "9c4dfb1e-0e02-4367-df40-eb28b5ac43e7"
      },
      "source": [
        "history = model.fit_generator(train_data_generator,\n",
        "                             steps_per_epoch=steps_per_epoch,\n",
        "                             epochs=20,\n",
        "                             verbose=1,\n",
        "                             validation_data=val_data_generator,\n",
        "                             validation_steps=validation_steps,\n",
        "                             class_weight=class_weights,\n",
        "                             callbacks=callbacks,\n",
        "                             workers=16)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "33/33 [==============================] - 76s 2s/step - loss: 0.2364 - acc: 0.9021 - val_loss: 1.6341 - val_acc: 0.5625\n",
            "Epoch 2/20\n",
            "33/33 [==============================] - 77s 2s/step - loss: 0.2419 - acc: 0.9034 - val_loss: 2.0252 - val_acc: 0.5625\n",
            "Epoch 3/20\n",
            "33/33 [==============================] - 78s 2s/step - loss: 0.2371 - acc: 0.9021 - val_loss: 1.6619 - val_acc: 0.6250\n",
            "Epoch 4/20\n",
            "33/33 [==============================] - 77s 2s/step - loss: 0.2445 - acc: 0.9019 - val_loss: 1.6355 - val_acc: 0.5000\n",
            "Epoch 5/20\n",
            "33/33 [==============================] - 77s 2s/step - loss: 0.2418 - acc: 0.9021 - val_loss: 1.4047 - val_acc: 0.5000\n",
            "Epoch 6/20\n",
            "33/33 [==============================] - 78s 2s/step - loss: 0.2498 - acc: 0.8998 - val_loss: 2.8264 - val_acc: 0.6250\n",
            "Epoch 7/20\n",
            "33/33 [==============================] - 78s 2s/step - loss: 0.2323 - acc: 0.9063 - val_loss: 1.6799 - val_acc: 0.6250\n",
            "Epoch 8/20\n",
            "33/33 [==============================] - 78s 2s/step - loss: 0.2400 - acc: 0.9052 - val_loss: 2.6692 - val_acc: 0.5625\n",
            "Epoch 9/20\n",
            "33/33 [==============================] - 78s 2s/step - loss: 0.2278 - acc: 0.9076 - val_loss: 2.4193 - val_acc: 0.5000\n",
            "Epoch 10/20\n",
            "32/33 [============================>.] - ETA: 2s - loss: 0.2305 - acc: 0.9023\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.999998695775504e-09.\n",
            "33/33 [==============================] - 78s 2s/step - loss: 0.2295 - acc: 0.9036 - val_loss: 1.4943 - val_acc: 0.6250\n",
            "Epoch 11/20\n",
            "33/33 [==============================] - 78s 2s/step - loss: 0.2304 - acc: 0.9071 - val_loss: 2.2483 - val_acc: 0.5625\n",
            "Epoch 12/20\n",
            "33/33 [==============================] - 78s 2s/step - loss: 0.2324 - acc: 0.9029 - val_loss: 2.1008 - val_acc: 0.5000\n",
            "Epoch 13/20\n",
            "33/33 [==============================] - 78s 2s/step - loss: 0.2494 - acc: 0.9017 - val_loss: 1.2278 - val_acc: 0.6250\n",
            "Epoch 14/20\n",
            "33/33 [==============================] - 79s 2s/step - loss: 0.2374 - acc: 0.8989 - val_loss: 2.0448 - val_acc: 0.5625\n",
            "Epoch 15/20\n",
            "33/33 [==============================] - 79s 2s/step - loss: 0.2451 - acc: 0.8998 - val_loss: 1.5472 - val_acc: 0.5000\n",
            "Epoch 16/20\n",
            "33/33 [==============================] - 79s 2s/step - loss: 0.2429 - acc: 0.9048 - val_loss: 1.1409 - val_acc: 0.6250\n",
            "Epoch 17/20\n",
            "33/33 [==============================] - 78s 2s/step - loss: 0.2322 - acc: 0.9063 - val_loss: 1.9582 - val_acc: 0.4375\n",
            "Epoch 18/20\n",
            "33/33 [==============================] - 79s 2s/step - loss: 0.2361 - acc: 0.9042 - val_loss: 1.8251 - val_acc: 0.6250\n",
            "Epoch 19/20\n",
            "33/33 [==============================] - 78s 2s/step - loss: 0.2516 - acc: 0.8987 - val_loss: 1.7699 - val_acc: 0.5625\n",
            "Epoch 20/20\n",
            "33/33 [==============================] - 78s 2s/step - loss: 0.2247 - acc: 0.9111 - val_loss: 1.6132 - val_acc: 0.5625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJTN426SYTxg",
        "colab_type": "text"
      },
      "source": [
        "Model has been run for 20 epochs here. Although it was tried for 80 epochs, the val_loss did not show consistent reduction for some reason"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsGj85NG1aV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_file_name = 'pneumonia_trained.hd5'\n",
        "model.save(os.path.join(model_dir,model_file_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dq5Cr98NRy7W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "24edea59-cd1d-4155-b642-39e537183488"
      },
      "source": [
        "loaded_model = tf.keras.models.load_model(os.path.join(model_dir,model_file_name))\n",
        "result = loaded_model.evaluate_generator(test_data_generator, steps=len(test_data_generator),verbose=1)\n",
        "print('Loss: ', result[0])\n",
        "print('Accuracy: ', result[1])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r1/1 [==============================] - 14s 14s/step - loss: 1.2819 - acc: 0.6843\n",
            "Loss:  1.2818865776062012\n",
            "Accuracy:  0.6842949\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3IjCn_kSMl7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "28f0daea-10c1-4bba-e228-f4ad99c9a8bf"
      },
      "source": [
        "import random\n",
        "num_batches = len(test_data_generator)\n",
        "random_batch = random.randint(0,num_batches-1)\n",
        "y_img_batch, y_true_batch = test_data_generator[random_batch]\n",
        "y_pred_batch = loaded_model.predict(y_img_batch)\n",
        "batch_size = len(y_true_batch)\n",
        "print('batch Size: ', batch_size)\n",
        "print('Accuracy: ', np.mean(y_pred_batch.argmax(axis=-1)==y_true_batch.argmax(axis=-1))*100)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch Size:  624\n",
            "Accuracy:  69.87179487179486\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}